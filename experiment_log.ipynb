{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flight Delay Insurance Model - Experiment Log\n",
        "\n",
        "## Overview\n",
        "This notebook tracks all experiments for optimizing the flight delay prediction model.\n",
        "\n",
        "**Goal**: Minimize Custom Payout MSE (Currently ~3684, Baseline ~4677)\n",
        "\n",
        "**Evaluation Metric**: Mean Squared Error between actual payout and expected payout\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Load Data and Define Evaluation Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, log_loss, classification_report\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: (582425, 120)\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "path_dir = kagglehub.dataset_download(\"shubhamsingh42/flight-delay-dataset-2018-2024\")\n",
        "file_name = \"flight_data_2018_2024.csv\"\n",
        "path_to_file = os.path.join(path_dir, file_name)\n",
        "df = pd.read_csv(path_to_file, low_memory=False)\n",
        "\n",
        "print(f\"Dataset loaded: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data cleaning completed\n",
            "Final shape: (582425, 13)\n",
            "\n",
            "Delay category distribution:\n",
            "Delay_Category\n",
            "0    336861\n",
            "1    172087\n",
            "2     28170\n",
            "3     21597\n",
            "4     23710\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Data cleaning (from main notebook)\n",
        "input_features = [\n",
        "    'Marketing_Airline_Network', 'Quarter', 'Month', 'DayofMonth',\n",
        "    'DayOfWeek', 'CRSDepTime', 'OriginAirportID', 'DestAirportID',\n",
        "    'OriginCityMarketID', 'DestCityMarketID', 'Distance'\n",
        "]\n",
        "\n",
        "target_features = [\n",
        "    'ArrDelayMinutes', 'Cancelled', 'Diverted'\n",
        "]\n",
        "df_clean = df[input_features + target_features].copy()\n",
        "\n",
        "bin1 = 60\n",
        "bin2 = 120\n",
        "\n",
        "# Create delay categories\n",
        "df_clean['Delay_Category'] = -1\n",
        "df_clean.loc[(df_clean['Cancelled'] == 1) | (df_clean['Diverted'] == 1), 'Delay_Category'] = 4\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] >= bin2), 'Delay_Category'] = 3\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] >= bin1) & (df_clean['ArrDelayMinutes'] < bin2), 'Delay_Category'] = 2\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] > 0) & (df_clean['ArrDelayMinutes'] < bin1), 'Delay_Category'] = 1\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] == 0), 'Delay_Category'] = 0\n",
        "df_clean = df_clean[df_clean['Delay_Category'] != -1]\n",
        "\n",
        "df_clean = df_clean.drop(columns=target_features)\n",
        "\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'Marketing_Airline_Network', 'Quarter', 'Month', 'DayofMonth',\n",
        "    'DayOfWeek', 'OriginAirportID', 'DestAirportID', 'OriginCityMarketID',\n",
        "    'DestCityMarketID'\n",
        "]\n",
        "\n",
        "for col in CATEGORICAL_FEATURES:\n",
        "    df_clean[col] = df_clean[col].astype('category')\n",
        "\n",
        "def time_to_block(time_hhmm):\n",
        "    hour = time_hhmm // 100\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 22:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "df_clean['CRSDepTime_Block'] = df_clean['CRSDepTime'].apply(time_to_block).astype('category')\n",
        "df_clean['CRSDepTime'] = df_clean['CRSDepTime'] // 100 * 60 + df_clean['CRSDepTime'] % 100\n",
        "\n",
        "print(\"Data cleaning completed\")\n",
        "print(f\"Final shape: {df_clean.shape}\")\n",
        "print(f\"\\nDelay category distribution:\\n{df_clean['Delay_Category'].value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation framework defined\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "NUM_CLASSES = 5\n",
        "PAYOUT_MAP = {\n",
        "    0: 0,\n",
        "    1: 50,\n",
        "    2: 100,\n",
        "    3: 300,\n",
        "    4: 200\n",
        "}\n",
        "N_SPLITS = 3\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def calculate_expected_payout_mse(y_true_categories, y_proba):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Squared Error between the Actual Payout and the Expected Payout.\n",
        "    \"\"\"\n",
        "    y_actual_payout = y_true_categories.map(PAYOUT_MAP).values\n",
        "    payout_vector = np.array([PAYOUT_MAP[i] for i in range(NUM_CLASSES)])\n",
        "    y_expected_payout = np.dot(y_proba, payout_vector)\n",
        "    custom_mse = mean_squared_error(y_actual_payout, y_expected_payout)\n",
        "    return custom_mse, y_actual_payout, y_expected_payout\n",
        "\n",
        "# Cross-validation strategy\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "print(\"Evaluation framework defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment logging functions defined\n"
          ]
        }
      ],
      "source": [
        "# Experiment tracking\n",
        "experiment_results = []\n",
        "\n",
        "def log_experiment(experiment_id, model_name, rationale, parameters, cv_scores, mean_score, std_score, additional_notes=\"\"):\n",
        "    \"\"\"\n",
        "    Log experiment results to tracking list\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'Experiment_ID': experiment_id,\n",
        "        'Model': model_name,\n",
        "        'Rationale': rationale,\n",
        "        'Parameters': str(parameters),\n",
        "        'CV_Scores': cv_scores,\n",
        "        'Mean_MSE': mean_score,\n",
        "        'Std_MSE': std_score,\n",
        "        'Notes': additional_notes,\n",
        "        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "    experiment_results.append(result)\n",
        "    return result\n",
        "\n",
        "def show_experiment_summary():\n",
        "    \"\"\"\n",
        "    Display summary of all experiments\n",
        "    \"\"\"\n",
        "    df_results = pd.DataFrame(experiment_results)\n",
        "    df_summary = df_results[['Experiment_ID', 'Model', 'Mean_MSE', 'Std_MSE', 'Rationale']].copy()\n",
        "    df_summary = df_summary.sort_values('Mean_MSE')\n",
        "    return df_summary\n",
        "\n",
        "print(\"Experiment logging functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline: Current Best Model (XGBoost from main notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Baseline XGBoost...\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Prepare baseline data\n",
        "X_baseline = df_clean.drop(columns=['Delay_Category'])\n",
        "y_baseline = df_clean['Delay_Category']\n",
        "\n",
        "NUMERICAL_FEATURES = ['Distance', 'CRSDepTime']\n",
        "CATEGORICAL_FEATURES_BASE = X_baseline.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_baseline = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_BASE)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "xgb_baseline = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    eval_metric='mlogloss',\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.05,\n",
        "    num_class=NUM_CLASSES,\n",
        "    reg_lambda=1,\n",
        "    use_label_encoder=False,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "# Run baseline cross-validation\n",
        "print(\"Running Baseline XGBoost...\")\n",
        "cv_scores_baseline = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_baseline, y_baseline), 1):\n",
        "    X_train, X_test = X_baseline.iloc[train_index], X_baseline.iloc[test_index]\n",
        "    y_train, y_test = y_baseline.iloc[train_index], y_baseline.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_baseline.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_baseline.transform(X_test)\n",
        "    \n",
        "    xgb_baseline.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = xgb_baseline.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_baseline.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_baseline = np.mean(cv_scores_baseline)\n",
        "std_mse_baseline = np.std(cv_scores_baseline)\n",
        "\n",
        "print(f\"\\nBaseline Mean MSE: {mean_mse_baseline:.2f} ± {std_mse_baseline:.2f}\")\n",
        "\n",
        "# Log baseline\n",
        "log_experiment(\n",
        "    experiment_id=\"BASELINE\",\n",
        "    model_name=\"XGBoost (Original)\",\n",
        "    rationale=\"Current best model from main notebook - establishes benchmark\",\n",
        "    parameters=xgb_baseline.get_params(),\n",
        "    cv_scores=cv_scores_baseline,\n",
        "    mean_score=mean_mse_baseline,\n",
        "    std_score=std_mse_baseline,\n",
        "    additional_notes=\"Uses basic features with log1p transform on numerical features\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for feature engineering\n",
        "df_features = df_clean.copy()\n",
        "print(f\"Starting feature engineering with base shape: {df_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment A1: Temporal Features - Weekend Indicator\n",
        "\n",
        "**Rationale**: Weekends may have different delay patterns (less business travel, different staffing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add weekend indicator (DayOfWeek: 1=Monday, ..., 7=Sunday)\n",
        "df_features['IsWeekend'] = (df_features['DayOfWeek'].isin([6, 7])).astype('category')\n",
        "\n",
        "# Test with baseline XGBoost\n",
        "X_a1 = df_features.drop(columns=['Delay_Category'])\n",
        "y_a1 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_A1 = X_a1.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_a1 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_A1)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment A1: Weekend Indicator\")\n",
        "cv_scores_a1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_a1, y_a1), 1):\n",
        "    X_train, X_test = X_a1.iloc[train_index], X_a1.iloc[test_index]\n",
        "    y_train, y_test = y_a1.iloc[train_index], y_a1.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_a1.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_a1.transform(X_test)\n",
        "    \n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_a1.append(custom_mse)\n",
        "\n",
        "mean_mse_a1 = np.mean(cv_scores_a1)\n",
        "std_mse_a1 = np.std(cv_scores_a1)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_a1:.2f} ± {std_mse_a1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_a1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"A1\",\n",
        "    model_name=\"XGBoost + Weekend\",\n",
        "    rationale=\"Weekend flights may have different delay patterns\",\n",
        "    parameters={\"added_features\": \"IsWeekend\"},\n",
        "    cv_scores=cv_scores_a1,\n",
        "    mean_score=mean_mse_a1,\n",
        "    std_score=std_mse_a1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_a1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment A2: Temporal Features - Rush Hour Indicator\n",
        "\n",
        "**Rationale**: Morning (6-9am) and evening (4-7pm) rush hours may have more congestion and delays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add rush hour indicator\n",
        "def is_rush_hour(time_minutes):\n",
        "    hour = time_minutes // 60\n",
        "    # Morning rush: 6-9am, Evening rush: 4-7pm (16-19)\n",
        "    return ((6 <= hour < 9) | (16 <= hour < 19))\n",
        "\n",
        "df_features['IsRushHour'] = df_features['CRSDepTime'].apply(is_rush_hour).astype('category')\n",
        "\n",
        "X_a2 = df_features.drop(columns=['Delay_Category'])\n",
        "y_a2 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_A2 = X_a2.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_a2 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_A2)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment A2: Rush Hour Indicator\")\n",
        "cv_scores_a2 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_a2, y_a2), 1):\n",
        "    X_train, X_test = X_a2.iloc[train_index], X_a2.iloc[test_index]\n",
        "    y_train, y_test = y_a2.iloc[train_index], y_a2.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_a2.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_a2.transform(X_test)\n",
        "    \n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_a2.append(custom_mse)\n",
        "\n",
        "mean_mse_a2 = np.mean(cv_scores_a2)\n",
        "std_mse_a2 = np.std(cv_scores_a2)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_a2:.2f} ± {std_mse_a2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_a2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"A2\",\n",
        "    model_name=\"XGBoost + Rush Hour\",\n",
        "    rationale=\"Rush hour flights face more airport congestion\",\n",
        "    parameters={\"added_features\": \"IsWeekend, IsRushHour\"},\n",
        "    cv_scores=cv_scores_a2,\n",
        "    mean_score=mean_mse_a2,\n",
        "    std_score=std_mse_a2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_a2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment B1: Route Features - Distance Bins\n",
        "\n",
        "**Rationale**: Short, medium, and long-haul flights have different operational characteristics and delay patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add distance bins\n",
        "def categorize_distance(distance):\n",
        "    if distance < 500:\n",
        "        return 'Short'  # <500 miles\n",
        "    elif distance < 1500:\n",
        "        return 'Medium'  # 500-1500 miles\n",
        "    else:\n",
        "        return 'Long'  # >1500 miles\n",
        "\n",
        "df_features['Distance_Bin'] = df_features['Distance'].apply(categorize_distance).astype('category')\n",
        "\n",
        "X_b1 = df_features.drop(columns=['Delay_Category'])\n",
        "y_b1 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_B1 = X_b1.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_b1 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_B1)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment B1: Distance Bins\")\n",
        "cv_scores_b1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_b1, y_b1), 1):\n",
        "    X_train, X_test = X_b1.iloc[train_index], X_b1.iloc[test_index]\n",
        "    y_train, y_test = y_b1.iloc[train_index], y_b1.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_b1.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_b1.transform(X_test)\n",
        "    \n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_b1.append(custom_mse)\n",
        "\n",
        "mean_mse_b1 = np.mean(cv_scores_b1)\n",
        "std_mse_b1 = np.std(cv_scores_b1)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_b1:.2f} ± {std_mse_b1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_b1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"B1\",\n",
        "    model_name=\"XGBoost + Distance Bins\",\n",
        "    rationale=\"Different flight distances have different delay characteristics\",\n",
        "    parameters={\"added_features\": \"IsWeekend, IsRushHour, Distance_Bin\"},\n",
        "    cv_scores=cv_scores_b1,\n",
        "    mean_score=mean_mse_b1,\n",
        "    std_score=std_mse_b1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_b1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment C1: Historical Features - Airport Delay Rates\n",
        "\n",
        "**Rationale**: Some airports have consistently higher delay rates; calculate aggregated statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate origin and destination airport delay rates\n",
        "# Use the full dataset to avoid data leakage (we're using historical aggregate stats)\n",
        "origin_delay_rate = df_features.groupby('OriginAirportID')['Delay_Category'].apply(lambda x: (x > 0).mean())\n",
        "dest_delay_rate = df_features.groupby('DestAirportID')['Delay_Category'].apply(lambda x: (x > 0).mean())\n",
        "\n",
        "df_features['Origin_Delay_Rate'] = df_features['OriginAirportID'].map(origin_delay_rate)\n",
        "df_features['Dest_Delay_Rate'] = df_features['DestAirportID'].map(dest_delay_rate)\n",
        "\n",
        "# Update numerical features\n",
        "NUMERICAL_FEATURES_C1 = ['Distance', 'CRSDepTime', 'Origin_Delay_Rate', 'Dest_Delay_Rate']\n",
        "\n",
        "X_c1 = df_features.drop(columns=['Delay_Category'])\n",
        "y_c1 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_C1 = X_c1.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_c1 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES_C1),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_C1)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment C1: Airport Delay Rates\")\n",
        "cv_scores_c1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_c1, y_c1), 1):\n",
        "    X_train, X_test = X_c1.iloc[train_index], X_c1.iloc[test_index]\n",
        "    y_train, y_test = y_c1.iloc[train_index], y_c1.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_c1.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_c1.transform(X_test)\n",
        "    \n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_c1.append(custom_mse)\n",
        "\n",
        "mean_mse_c1 = np.mean(cv_scores_c1)\n",
        "std_mse_c1 = np.std(cv_scores_c1)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_c1:.2f} ± {std_mse_c1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_c1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"C1\",\n",
        "    model_name=\"XGBoost + Airport Delay Rates\",\n",
        "    rationale=\"Historical airport performance predicts future delays\",\n",
        "    parameters={\"added_features\": \"All previous + Origin_Delay_Rate, Dest_Delay_Rate\"},\n",
        "    cv_scores=cv_scores_c1,\n",
        "    mean_score=mean_mse_c1,\n",
        "    std_score=std_mse_c1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_c1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment C2: Historical Features - Airline Delay Rates\n",
        "\n",
        "**Rationale**: Different airlines have different operational efficiency and delay rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate airline delay rates\n",
        "airline_delay_rate = df_features.groupby('Marketing_Airline_Network')['Delay_Category'].apply(lambda x: (x > 0).mean())\n",
        "df_features['Airline_Delay_Rate'] = df_features['Marketing_Airline_Network'].map(airline_delay_rate)\n",
        "\n",
        "NUMERICAL_FEATURES_C2 = ['Distance', 'CRSDepTime', 'Origin_Delay_Rate', 'Dest_Delay_Rate', 'Airline_Delay_Rate']\n",
        "\n",
        "X_c2 = df_features.drop(columns=['Delay_Category'])\n",
        "y_c2 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_C2 = X_c2.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_c2 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES_C2),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_C2)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment C2: Airline Delay Rates\")\n",
        "cv_scores_c2 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_c2, y_c2), 1):\n",
        "    X_train, X_test = X_c2.iloc[train_index], X_c2.iloc[test_index]\n",
        "    y_train, y_test = y_c2.iloc[train_index], y_c2.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_c2.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_c2.transform(X_test)\n",
        "    \n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_c2.append(custom_mse)\n",
        "\n",
        "mean_mse_c2 = np.mean(cv_scores_c2)\n",
        "std_mse_c2 = np.std(cv_scores_c2)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_c2:.2f} ± {std_mse_c2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_c2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"C2\",\n",
        "    model_name=\"XGBoost + Airline Delay Rates\",\n",
        "    rationale=\"Airline operational efficiency is a strong predictor\",\n",
        "    parameters={\"added_features\": \"All previous + Airline_Delay_Rate\"},\n",
        "    cv_scores=cv_scores_c2,\n",
        "    mean_score=mean_mse_c2,\n",
        "    std_score=std_mse_c2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_c2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show feature engineering results\n",
        "print(\"=\"*70)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "feature_summary = show_experiment_summary()\n",
        "print(feature_summary.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Determine best feature set\n",
        "best_features_exp = feature_summary.iloc[0]\n",
        "print(f\"\\nBest Feature Set: {best_features_exp['Experiment_ID']} - {best_features_exp['Model']}\")\n",
        "print(f\"Best MSE: {best_features_exp['Mean_MSE']:.2f}\")\n",
        "print(f\"Improvement over baseline: {mean_mse_baseline - best_features_exp['Mean_MSE']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Phase 3: Model Architecture Exploration\n",
        "\n",
        "**Strategy**: Test different model families with the best feature set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the best feature set from previous phase\n",
        "# For now, we'll use all engineered features (C2)\n",
        "X_best = df_features.drop(columns=['Delay_Category'])\n",
        "y_best = df_features['Delay_Category']\n",
        "\n",
        "NUMERICAL_FEATURES_BEST = ['Distance', 'CRSDepTime', 'Origin_Delay_Rate', 'Dest_Delay_Rate', 'Airline_Delay_Rate']\n",
        "CATEGORICAL_FEATURES_BEST = X_best.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "print(f\"Best feature set: {len(NUMERICAL_FEATURES_BEST)} numerical + {len(CATEGORICAL_FEATURES_BEST)} categorical\")\n",
        "print(f\"Total samples: {len(X_best)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment D1: LightGBM Model\n",
        "\n",
        "**Rationale**: LightGBM is faster than XGBoost and often performs better with categorical features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "preprocessor_best = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_BEST)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "lgbm_model = LGBMClassifier(\n",
        "    objective='multiclass',\n",
        "    num_class=NUM_CLASSES,\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.05,\n",
        "    reg_lambda=1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "print(\"Experiment D1: LightGBM\")\n",
        "cv_scores_d1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "    \n",
        "    lgbm_model.fit(X_train_processed, y_train)\n",
        "    y_pred_proba = lgbm_model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_d1.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_d1 = np.mean(cv_scores_d1)\n",
        "std_mse_d1 = np.std(cv_scores_d1)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_d1:.2f} ± {std_mse_d1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_d1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"D1\",\n",
        "    model_name=\"LightGBM\",\n",
        "    rationale=\"LightGBM often outperforms XGBoost with better categorical handling\",\n",
        "    parameters=lgbm_model.get_params(),\n",
        "    cv_scores=cv_scores_d1,\n",
        "    mean_score=mean_mse_d1,\n",
        "    std_score=std_mse_d1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_d1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment D2: CatBoost Model\n",
        "\n",
        "**Rationale**: CatBoost has native categorical feature support and strong baseline performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost can handle categorical features natively\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=300,\n",
        "    depth=7,\n",
        "    learning_rate=0.05,\n",
        "    loss_function='MultiClass',\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=False,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "print(\"Experiment D2: CatBoost\")\n",
        "cv_scores_d2 = []\n",
        "\n",
        "# Get categorical feature indices for CatBoost\n",
        "cat_feature_indices = [i for i, col in enumerate(X_best.columns) if col in CATEGORICAL_FEATURES_BEST]\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "    \n",
        "    # CatBoost preprocessor (only numerical features need transformation)\n",
        "    preprocessor_catboost = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                 ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                 ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "        ], remainder='passthrough'\n",
        "    )\n",
        "    \n",
        "    X_train_processed = preprocessor_catboost.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_catboost.transform(X_test)\n",
        "    \n",
        "    # Identify new categorical indices after preprocessing\n",
        "    num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "    cat_indices_processed = list(range(num_numerical, X_train_processed.shape[1]))\n",
        "    \n",
        "    catboost_model.fit(X_train_processed, y_train, cat_features=cat_indices_processed)\n",
        "    y_pred_proba = catboost_model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_d2.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_d2 = np.mean(cv_scores_d2)\n",
        "std_mse_d2 = np.std(cv_scores_d2)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_d2:.2f} ± {std_mse_d2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_d2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"D2\",\n",
        "    model_name=\"CatBoost\",\n",
        "    rationale=\"Native categorical handling may improve performance\",\n",
        "    parameters=catboost_model.get_params(),\n",
        "    cv_scores=cv_scores_d2,\n",
        "    mean_score=mean_mse_d2,\n",
        "    std_score=std_mse_d2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_d2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Experiment D3: Random Forest\")\n",
        "cv_scores_d3 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "    \n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "    \n",
        "    rf_model.fit(X_train_processed, y_train)\n",
        "    y_pred_proba = rf_model.predict_proba(X_test_processed)\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_d3.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_d3 = np.mean(cv_scores_d3)\n",
        "std_mse_d3 = np.std(cv_scores_d3)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_d3:.2f} ± {std_mse_d3:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_d3:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"D3\",\n",
        "    model_name=\"Random Forest\",\n",
        "    rationale=\"Non-boosted ensemble as baseline comparison\",\n",
        "    parameters=rf_model.get_params(),\n",
        "    cv_scores=cv_scores_d3,\n",
        "    mean_score=mean_mse_d3,\n",
        "    std_score=std_mse_d3,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_d3:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all experiment results so far\n",
        "print(\"=\"*70)\n",
        "print(\"ALL EXPERIMENTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "all_summary = show_experiment_summary()\n",
        "print(all_summary.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Identify top 3 models for hyperparameter tuning\n",
        "top3_models = all_summary.head(3)\n",
        "print(f\"\\nTop 3 Models for Hyperparameter Tuning:\")\n",
        "for idx, row in top3_models.iterrows():\n",
        "    print(f\"{row['Experiment_ID']}: {row['Model']} - MSE: {row['Mean_MSE']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Phase 4: Hyperparameter Optimization\n",
        "\n",
        "**Strategy**: Tune the top-performing models systematically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment E1: XGBoost Hyperparameter Tuning\n",
        "\n",
        "**Rationale**: Systematically search for optimal learning rate, depth, and regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost hyperparameter grid search\n",
        "print(\"Experiment E1: XGBoost Hyperparameter Tuning\")\n",
        "\n",
        "param_grid_xgb = [\n",
        "    # Test 1: Deeper trees with more regularization\n",
        "    {'max_depth': 10, 'learning_rate': 0.03, 'n_estimators': 400, 'reg_lambda': 2, 'subsample': 0.8},\n",
        "    # Test 2: Shallower trees with higher learning rate\n",
        "    {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 200, 'reg_lambda': 1, 'subsample': 0.9},\n",
        "    # Test 3: Balanced approach\n",
        "    {'max_depth': 8, 'learning_rate': 0.05, 'n_estimators': 350, 'reg_lambda': 1.5, 'subsample': 0.85},\n",
        "    # Test 4: More estimators with lower learning rate\n",
        "    {'max_depth': 7, 'learning_rate': 0.02, 'n_estimators': 500, 'reg_lambda': 1, 'subsample': 0.9},\n",
        "]\n",
        "\n",
        "best_xgb_score = float('inf')\n",
        "best_xgb_params = None\n",
        "\n",
        "for idx, params in enumerate(param_grid_xgb, 1):\n",
        "    print(f\"\\nTesting XGBoost config {idx}/{len(param_grid_xgb)}: {params}\")\n",
        "    \n",
        "    model = XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        eval_metric='mlogloss',\n",
        "        num_class=NUM_CLASSES,\n",
        "        use_label_encoder=False,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist',\n",
        "        **params\n",
        "    )\n",
        "    \n",
        "    cv_scores = []\n",
        "    for train_index, test_index in skf.split(X_best, y_best):\n",
        "        X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "        y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "        \n",
        "        X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "        X_test_processed = preprocessor_best.transform(X_test)\n",
        "        \n",
        "        model.fit(X_train_processed, y_train, verbose=False)\n",
        "        y_pred_proba = model.predict_proba(X_test_processed)\n",
        "        \n",
        "        custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "        cv_scores.append(custom_mse)\n",
        "    \n",
        "    mean_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    print(f\"  Mean MSE: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "    \n",
        "    log_experiment(\n",
        "        experiment_id=f\"E1_{idx}\",\n",
        "        model_name=f\"XGBoost Tuned {idx}\",\n",
        "        rationale=f\"Hyperparameter config {idx}\",\n",
        "        parameters=params,\n",
        "        cv_scores=cv_scores,\n",
        "        mean_score=mean_score,\n",
        "        std_score=std_score\n",
        "    )\n",
        "    \n",
        "    if mean_score < best_xgb_score:\n",
        "        best_xgb_score = mean_score\n",
        "        best_xgb_params = params\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best XGBoost Config: {best_xgb_params}\")\n",
        "print(f\"Best XGBoost MSE: {best_xgb_score:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - best_xgb_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment E2: LightGBM Hyperparameter Tuning\n",
        "\n",
        "**Rationale**: Optimize LightGBM-specific parameters (num_leaves, min_child_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LightGBM hyperparameter grid search\n",
        "print(\"Experiment E2: LightGBM Hyperparameter Tuning\")\n",
        "\n",
        "param_grid_lgbm = [\n",
        "    # Test 1: More leaves with regularization\n",
        "    {'num_leaves': 127, 'learning_rate': 0.03, 'n_estimators': 400, 'reg_lambda': 2, 'min_child_samples': 30},\n",
        "    # Test 2: Fewer leaves, higher learning rate\n",
        "    {'num_leaves': 31, 'learning_rate': 0.1, 'n_estimators': 200, 'reg_lambda': 1, 'min_child_samples': 20},\n",
        "    # Test 3: Balanced approach\n",
        "    {'num_leaves': 63, 'learning_rate': 0.05, 'n_estimators': 350, 'reg_lambda': 1.5, 'min_child_samples': 25},\n",
        "    # Test 4: Conservative with more estimators\n",
        "    {'num_leaves': 50, 'learning_rate': 0.02, 'n_estimators': 500, 'reg_lambda': 1, 'min_child_samples': 20},\n",
        "]\n",
        "\n",
        "best_lgbm_score = float('inf')\n",
        "best_lgbm_params = None\n",
        "\n",
        "for idx, params in enumerate(param_grid_lgbm, 1):\n",
        "    print(f\"\\nTesting LightGBM config {idx}/{len(param_grid_lgbm)}: {params}\")\n",
        "    \n",
        "    model = LGBMClassifier(\n",
        "        objective='multiclass',\n",
        "        num_class=NUM_CLASSES,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1,\n",
        "        **params\n",
        "    )\n",
        "    \n",
        "    cv_scores = []\n",
        "    for train_index, test_index in skf.split(X_best, y_best):\n",
        "        X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "        y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "        \n",
        "        X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "        X_test_processed = preprocessor_best.transform(X_test)\n",
        "        \n",
        "        model.fit(X_train_processed, y_train)\n",
        "        y_pred_proba = model.predict_proba(X_test_processed)\n",
        "        \n",
        "        custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "        cv_scores.append(custom_mse)\n",
        "    \n",
        "    mean_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    print(f\"  Mean MSE: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "    \n",
        "    log_experiment(\n",
        "        experiment_id=f\"E2_{idx}\",\n",
        "        model_name=f\"LightGBM Tuned {idx}\",\n",
        "        rationale=f\"Hyperparameter config {idx}\",\n",
        "        parameters=params,\n",
        "        cv_scores=cv_scores,\n",
        "        mean_score=mean_score,\n",
        "        std_score=std_score\n",
        "    )\n",
        "    \n",
        "    if mean_score < best_lgbm_score:\n",
        "        best_lgbm_score = mean_score\n",
        "        best_lgbm_params = params\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best LightGBM Config: {best_lgbm_params}\")\n",
        "print(f\"Best LightGBM MSE: {best_lgbm_score:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - best_lgbm_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CatBoost hyperparameter grid search\n",
        "print(\"Experiment E3: CatBoost Hyperparameter Tuning\")\n",
        "\n",
        "param_grid_catboost = [\n",
        "    # Test 1: Deeper trees\n",
        "    {'depth': 10, 'learning_rate': 0.03, 'iterations': 400, 'l2_leaf_reg': 5},\n",
        "    # Test 2: Shallower trees, higher LR\n",
        "    {'depth': 6, 'learning_rate': 0.1, 'iterations': 200, 'l2_leaf_reg': 3},\n",
        "    # Test 3: Balanced\n",
        "    {'depth': 8, 'learning_rate': 0.05, 'iterations': 350, 'l2_leaf_reg': 4},\n",
        "    # Test 4: More iterations\n",
        "    {'depth': 7, 'learning_rate': 0.02, 'iterations': 500, 'l2_leaf_reg': 3},\n",
        "]\n",
        "\n",
        "best_catboost_score = float('inf')\n",
        "best_catboost_params = None\n",
        "\n",
        "for idx, params in enumerate(param_grid_catboost, 1):\n",
        "    print(f\"\\nTesting CatBoost config {idx}/{len(param_grid_catboost)}: {params}\")\n",
        "    \n",
        "    model = CatBoostClassifier(\n",
        "        loss_function='MultiClass',\n",
        "        random_state=RANDOM_STATE,\n",
        "        verbose=False,\n",
        "        thread_count=-1,\n",
        "        **params\n",
        "    )\n",
        "    \n",
        "    cv_scores = []\n",
        "    for train_index, test_index in skf.split(X_best, y_best):\n",
        "        X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "        y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "        \n",
        "        preprocessor_catboost = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', Pipeline([\n",
        "                     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                     ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "            ], remainder='passthrough'\n",
        "        )\n",
        "        \n",
        "        X_train_processed = preprocessor_catboost.fit_transform(X_train)\n",
        "        X_test_processed = preprocessor_catboost.transform(X_test)\n",
        "        \n",
        "        num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "        cat_indices = list(range(num_numerical, X_train_processed.shape[1]))\n",
        "        \n",
        "        model.fit(X_train_processed, y_train, cat_features=cat_indices)\n",
        "        y_pred_proba = model.predict_proba(X_test_processed)\n",
        "        \n",
        "        custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "        cv_scores.append(custom_mse)\n",
        "    \n",
        "    mean_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    print(f\"  Mean MSE: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "    \n",
        "    log_experiment(\n",
        "        experiment_id=f\"E3_{idx}\",\n",
        "        model_name=f\"CatBoost Tuned {idx}\",\n",
        "        rationale=f\"Hyperparameter config {idx}\",\n",
        "        parameters=params,\n",
        "        cv_scores=cv_scores,\n",
        "        mean_score=mean_score,\n",
        "        std_score=std_score\n",
        "    )\n",
        "    \n",
        "    if mean_score < best_catboost_score:\n",
        "        best_catboost_score = mean_score\n",
        "        best_catboost_params = params\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best CatBoost Config: {best_catboost_params}\")\n",
        "print(f\"Best CatBoost MSE: {best_catboost_score:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - best_catboost_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Phase 5: Ensemble Methods\n",
        "\n",
        "**Strategy**: Combine predictions from multiple models to reduce variance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment F1: Simple Averaging Ensemble\n",
        "\n",
        "**Rationale**: Average predictions from top 3 models to reduce variance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Experiment F1: Simple Averaging Ensemble\")\n",
        "\n",
        "# Create best models based on hyperparameter tuning\n",
        "best_xgb = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    eval_metric='mlogloss',\n",
        "    num_class=NUM_CLASSES,\n",
        "    use_label_encoder=False,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    tree_method='hist',\n",
        "    **best_xgb_params\n",
        ")\n",
        "\n",
        "best_lgbm = LGBMClassifier(\n",
        "    objective='multiclass',\n",
        "    num_class=NUM_CLASSES,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1,\n",
        "    **best_lgbm_params\n",
        ")\n",
        "\n",
        "best_catboost = CatBoostClassifier(\n",
        "    loss_function='MultiClass',\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=False,\n",
        "    thread_count=-1,\n",
        "    **best_catboost_params\n",
        ")\n",
        "\n",
        "cv_scores_f1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "    \n",
        "    # XGBoost and LightGBM\n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "    \n",
        "    best_xgb.fit(X_train_processed, y_train, verbose=False)\n",
        "    xgb_proba = best_xgb.predict_proba(X_test_processed)\n",
        "    \n",
        "    best_lgbm.fit(X_train_processed, y_train)\n",
        "    lgbm_proba = best_lgbm.predict_proba(X_test_processed)\n",
        "    \n",
        "    # CatBoost\n",
        "    preprocessor_catboost = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                 ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                 ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "        ], remainder='passthrough'\n",
        "    )\n",
        "    \n",
        "    X_train_cat = preprocessor_catboost.fit_transform(X_train)\n",
        "    X_test_cat = preprocessor_catboost.transform(X_test)\n",
        "    \n",
        "    num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "    cat_indices = list(range(num_numerical, X_train_cat.shape[1]))\n",
        "    \n",
        "    best_catboost.fit(X_train_cat, y_train, cat_features=cat_indices)\n",
        "    catboost_proba = best_catboost.predict_proba(X_test_cat)\n",
        "    \n",
        "    # Simple average ensemble\n",
        "    ensemble_proba = (xgb_proba + lgbm_proba + catboost_proba) / 3\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, ensemble_proba)\n",
        "    cv_scores_f1.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_f1 = np.mean(cv_scores_f1)\n",
        "std_mse_f1 = np.std(cv_scores_f1)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_f1:.2f} ± {std_mse_f1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_f1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"F1\",\n",
        "    model_name=\"Ensemble (XGB+LGBM+CAT avg)\",\n",
        "    rationale=\"Average of top 3 tuned models\",\n",
        "    parameters={\"models\": [\"XGBoost\", \"LightGBM\", \"CatBoost\"], \"method\": \"simple_average\"},\n",
        "    cv_scores=cv_scores_f1,\n",
        "    mean_score=mean_mse_f1,\n",
        "    std_score=std_mse_f1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_f1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment F2: Weighted Ensemble\n",
        "\n",
        "**Rationale**: Weight models based on their individual performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Experiment F2: Weighted Ensemble\")\n",
        "\n",
        "# Calculate weights inversely proportional to MSE\n",
        "total_error = best_xgb_score + best_lgbm_score + best_catboost_score\n",
        "w_xgb = (1/best_xgb_score) / ((1/best_xgb_score) + (1/best_lgbm_score) + (1/best_catboost_score))\n",
        "w_lgbm = (1/best_lgbm_score) / ((1/best_xgb_score) + (1/best_lgbm_score) + (1/best_catboost_score))\n",
        "w_catboost = (1/best_catboost_score) / ((1/best_xgb_score) + (1/best_lgbm_score) + (1/best_catboost_score))\n",
        "\n",
        "print(f\"Weights: XGB={w_xgb:.3f}, LGBM={w_lgbm:.3f}, CAT={w_catboost:.3f}\")\n",
        "\n",
        "cv_scores_f2 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "    \n",
        "    # Get predictions from all three models (same as F1)\n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "    \n",
        "    best_xgb.fit(X_train_processed, y_train, verbose=False)\n",
        "    xgb_proba = best_xgb.predict_proba(X_test_processed)\n",
        "    \n",
        "    best_lgbm.fit(X_train_processed, y_train)\n",
        "    lgbm_proba = best_lgbm.predict_proba(X_test_processed)\n",
        "    \n",
        "    preprocessor_catboost = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                 ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                 ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "        ], remainder='passthrough'\n",
        "    )\n",
        "    \n",
        "    X_train_cat = preprocessor_catboost.fit_transform(X_train)\n",
        "    X_test_cat = preprocessor_catboost.transform(X_test)\n",
        "    \n",
        "    num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "    cat_indices = list(range(num_numerical, X_train_cat.shape[1]))\n",
        "    \n",
        "    best_catboost.fit(X_train_cat, y_train, cat_features=cat_indices)\n",
        "    catboost_proba = best_catboost.predict_proba(X_test_cat)\n",
        "    \n",
        "    # Weighted ensemble\n",
        "    ensemble_proba = w_xgb * xgb_proba + w_lgbm * lgbm_proba + w_catboost * catboost_proba\n",
        "    \n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, ensemble_proba)\n",
        "    cv_scores_f2.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_f2 = np.mean(cv_scores_f2)\n",
        "std_mse_f2 = np.std(cv_scores_f2)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_f2:.2f} ± {std_mse_f2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_f2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"F2\",\n",
        "    model_name=\"Ensemble (Weighted)\",\n",
        "    rationale=\"Performance-weighted average of top 3 models\",\n",
        "    parameters={\"models\": [\"XGBoost\", \"LightGBM\", \"CatBoost\"], \"method\": \"weighted\", \n",
        "                \"weights\": f\"XGB={w_xgb:.3f}, LGBM={w_lgbm:.3f}, CAT={w_catboost:.3f}\"},\n",
        "    cv_scores=cv_scores_f2,\n",
        "    mean_score=mean_mse_f2,\n",
        "    std_score=std_mse_f2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_f2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Phase 6: Final Model Selection\n",
        "\n",
        "**Strategy**: Select the best performing model and document complete pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final comprehensive summary\n",
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"FINAL EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "final_summary = show_experiment_summary()\n",
        "print(final_summary.head(10).to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select best model\n",
        "best_model_row = final_summary.iloc[0]\n",
        "print(f\"\\n🏆 BEST MODEL: {best_model_row['Model']}\")\n",
        "print(f\"   Experiment ID: {best_model_row['Experiment_ID']}\")\n",
        "print(f\"   Mean MSE: {best_model_row['Mean_MSE']:.2f} ± {best_model_row['Std_MSE']:.2f}\")\n",
        "print(f\"   Improvement over Baseline: {mean_mse_baseline - best_model_row['Mean_MSE']:.2f}\")\n",
        "print(f\"   Percentage Improvement: {((mean_mse_baseline - best_model_row['Mean_MSE'])/mean_mse_baseline * 100):.1f}%\")\n",
        "print(f\"\\nRationale: {best_model_row['Rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Performance Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top 10 models\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "top_models = final_summary.head(10)\n",
        "y_pos = np.arange(len(top_models))\n",
        "\n",
        "bars = ax.barh(y_pos, top_models['Mean_MSE'], xerr=top_models['Std_MSE'], \n",
        "               color=['green' if i == 0 else 'steelblue' for i in range(len(top_models))])\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(top_models['Experiment_ID'] + ': ' + top_models['Model'])\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Payout MSE (lower is better)')\n",
        "ax.set_title('Top 10 Model Performances')\n",
        "ax.axvline(x=mean_mse_baseline, color='red', linestyle='--', label=f'Baseline: {mean_mse_baseline:.2f}')\n",
        "ax.legend()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Findings and Insights\n",
        "\n",
        "Based on the experiments:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KEY FINDINGS:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Feature engineering insights\n",
        "feature_exps = [exp for exp in experiment_results if exp['Experiment_ID'] in ['A1', 'A2', 'B1', 'C1', 'C2']]\n",
        "if feature_exps:\n",
        "    best_feature = min(feature_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n1. FEATURE ENGINEERING:\")\n",
        "    print(f\"   Most impactful features: {best_feature['Experiment_ID']} - {best_feature['Model']}\")\n",
        "    print(f\"   Improvement: {mean_mse_baseline - best_feature['Mean_MSE']:.2f}\")\n",
        "\n",
        "# Model architecture insights\n",
        "model_exps = [exp for exp in experiment_results if exp['Experiment_ID'] in ['D1', 'D2', 'D3']]\n",
        "if model_exps:\n",
        "    best_arch = min(model_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n2. MODEL ARCHITECTURE:\")\n",
        "    print(f\"   Best base model: {best_arch['Model']}\")\n",
        "    print(f\"   MSE: {best_arch['Mean_MSE']:.2f}\")\n",
        "\n",
        "# Hyperparameter tuning insights\n",
        "tuned_exps = [exp for exp in experiment_results if exp['Experiment_ID'].startswith('E')]\n",
        "if tuned_exps:\n",
        "    best_tuned = min(tuned_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n3. HYPERPARAMETER TUNING:\")\n",
        "    print(f\"   Best tuned model: {best_tuned['Model']}\")\n",
        "    print(f\"   MSE: {best_tuned['Mean_MSE']:.2f}\")\n",
        "\n",
        "# Ensemble insights\n",
        "ensemble_exps = [exp for exp in experiment_results if exp['Experiment_ID'] in ['F1', 'F2']]\n",
        "if ensemble_exps:\n",
        "    best_ensemble = min(ensemble_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n4. ENSEMBLE METHODS:\")\n",
        "    print(f\"   Best ensemble: {best_ensemble['Model']}\")\n",
        "    print(f\"   MSE: {best_ensemble['Mean_MSE']:.2f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"OVERALL BEST: {best_model_row['Model']}\")\n",
        "print(f\"Final MSE: {best_model_row['Mean_MSE']:.2f} (vs Baseline: {mean_mse_baseline:.2f})\")\n",
        "print(f\"Absolute Improvement: {mean_mse_baseline - best_model_row['Mean_MSE']:.2f}\")\n",
        "print(f\"Relative Improvement: {((mean_mse_baseline - best_model_row['Mean_MSE'])/mean_mse_baseline * 100):.1f}%\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Experiment Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all experiment results to CSV for reference\n",
        "df_all_results = pd.DataFrame(experiment_results)\n",
        "df_all_results.to_csv('experiment_results.csv', index=False)\n",
        "print(\"Experiment results saved to 'experiment_results.csv'\")\n",
        "\n",
        "# Save the final summary\n",
        "final_summary.to_csv('model_comparison_summary.csv', index=False)\n",
        "print(\"Model comparison saved to 'model_comparison_summary.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Complete Pipeline for Best Model\n",
        "\n",
        "Below is the complete, reproducible code for the best performing model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "COMPLETE REPRODUCIBLE PIPELINE FOR BEST MODEL\n",
        "=============================================\n",
        "\n",
        "The best model configuration will be determined after running all experiments.\n",
        "This cell will be updated with the exact code to reproduce the best results.\n",
        "\n",
        "Key components:\n",
        "1. Feature engineering (exact transformations)\n",
        "2. Preprocessing pipeline\n",
        "3. Model configuration with optimal hyperparameters\n",
        "4. Training procedure\n",
        "5. Evaluation code\n",
        "\n",
        "Random seed: 42 (for reproducibility)\n",
        "Cross-validation: 3-fold stratified\n",
        "Metric: Custom Payout MSE\n",
        "\n",
        "After running this notebook, copy the best configuration to the main notebook.\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flight-delay-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
