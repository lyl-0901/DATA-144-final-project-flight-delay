{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCjtNzKuc-3Z"
      },
      "source": [
        "# Flight Delay Insurance Model - Experiment Log\n",
        "\n",
        "## Overview\n",
        "This notebook tracks all experiments for optimizing the flight delay prediction model.\n",
        "\n",
        "**Goal**: Minimize Custom Payout MSE (Currently ~3684, Baseline ~4677)\n",
        "\n",
        "**Evaluation Metric**: Mean Squared Error between actual payout and expected payout\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5bMt2yYc-3b"
      },
      "source": [
        "## Setup: Load Data and Define Evaluation Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ozOQLIDZc-3b",
        "outputId": "96a7ab73-0f49-451c-a22c-58634e4cf7e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, log_loss, classification_report\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YlbIt8SYc-3c",
        "outputId": "4c991005-064e-4f8b-82e8-c2e0ba1084ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shubhamsingh42/flight-delay-dataset-2018-2024?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46.9M/46.9M [00:00<00:00, 70.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: (582425, 120)\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "path_dir = kagglehub.dataset_download(\"shubhamsingh42/flight-delay-dataset-2018-2024\")\n",
        "file_name = \"flight_data_2018_2024.csv\"\n",
        "path_to_file = os.path.join(path_dir, file_name)\n",
        "df = pd.read_csv(path_to_file, low_memory=False)\n",
        "\n",
        "print(f\"Dataset loaded: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7eis47Fyc-3c",
        "outputId": "1af87ccc-ea3f-4b2f-8afe-8119ca4dcc05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning completed\n",
            "Final shape: (582425, 13)\n",
            "\n",
            "Delay category distribution:\n",
            "Delay_Category\n",
            "0    336861\n",
            "1    172087\n",
            "2     28170\n",
            "3     21597\n",
            "4     23710\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Data cleaning (from main notebook)\n",
        "input_features = [\n",
        "    'Marketing_Airline_Network', 'Quarter', 'Month', 'DayofMonth',\n",
        "    'DayOfWeek', 'CRSDepTime', 'OriginAirportID', 'DestAirportID',\n",
        "    'OriginCityMarketID', 'DestCityMarketID', 'Distance'\n",
        "]\n",
        "\n",
        "target_features = [\n",
        "    'ArrDelayMinutes', 'Cancelled', 'Diverted'\n",
        "]\n",
        "df_clean = df[input_features + target_features].copy()\n",
        "\n",
        "bin1 = 60\n",
        "bin2 = 120\n",
        "\n",
        "# Create delay categories\n",
        "df_clean['Delay_Category'] = -1\n",
        "df_clean.loc[(df_clean['Cancelled'] == 1) | (df_clean['Diverted'] == 1), 'Delay_Category'] = 4\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] >= bin2), 'Delay_Category'] = 3\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] >= bin1) & (df_clean['ArrDelayMinutes'] < bin2), 'Delay_Category'] = 2\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] > 0) & (df_clean['ArrDelayMinutes'] < bin1), 'Delay_Category'] = 1\n",
        "df_clean.loc[(df_clean['Delay_Category'] != 4) & (df_clean['ArrDelayMinutes'] == 0), 'Delay_Category'] = 0\n",
        "df_clean = df_clean[df_clean['Delay_Category'] != -1]\n",
        "\n",
        "df_clean = df_clean.drop(columns=target_features)\n",
        "\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'Marketing_Airline_Network', 'Quarter', 'Month', 'DayofMonth',\n",
        "    'DayOfWeek', 'OriginAirportID', 'DestAirportID', 'OriginCityMarketID',\n",
        "    'DestCityMarketID'\n",
        "]\n",
        "\n",
        "for col in CATEGORICAL_FEATURES:\n",
        "    df_clean[col] = df_clean[col].astype('category')\n",
        "\n",
        "def time_to_block(time_hhmm):\n",
        "    hour = time_hhmm // 100\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 22:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "df_clean['CRSDepTime_Block'] = df_clean['CRSDepTime'].apply(time_to_block).astype('category')\n",
        "df_clean['CRSDepTime'] = df_clean['CRSDepTime'] // 100 * 60 + df_clean['CRSDepTime'] % 100\n",
        "\n",
        "print(\"Data cleaning completed\")\n",
        "print(f\"Final shape: {df_clean.shape}\")\n",
        "print(f\"\\nDelay category distribution:\\n{df_clean['Delay_Category'].value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxusAr8jc-3d"
      },
      "source": [
        "## Evaluation Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bRsm4pz7c-3d",
        "outputId": "86e3b223-ebb8-4149-f71c-f381b38a0a2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation framework defined\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "NUM_CLASSES = 5\n",
        "PAYOUT_MAP = {\n",
        "    0: 0,\n",
        "    1: 50,\n",
        "    2: 100,\n",
        "    3: 300,\n",
        "    4: 200\n",
        "}\n",
        "N_SPLITS = 3\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def calculate_expected_payout_mse(y_true_categories, y_proba):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Squared Error between the Actual Payout and the Expected Payout.\n",
        "    \"\"\"\n",
        "    y_actual_payout = y_true_categories.map(PAYOUT_MAP).values\n",
        "    payout_vector = np.array([PAYOUT_MAP[i] for i in range(NUM_CLASSES)])\n",
        "    y_expected_payout = np.dot(y_proba, payout_vector)\n",
        "    custom_mse = mean_squared_error(y_actual_payout, y_expected_payout)\n",
        "    return custom_mse, y_actual_payout, y_expected_payout\n",
        "\n",
        "# Cross-validation strategy\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "print(\"Evaluation framework defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PrNdtJ65c-3d",
        "outputId": "113f0480-29fb-4c36-ad87-d19fbe74cfbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment logging functions defined\n"
          ]
        }
      ],
      "source": [
        "# Experiment tracking\n",
        "experiment_results = []\n",
        "\n",
        "def log_experiment(experiment_id, model_name, rationale, parameters, cv_scores, mean_score, std_score, additional_notes=\"\"):\n",
        "    \"\"\"\n",
        "    Log experiment results to tracking list\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'Experiment_ID': experiment_id,\n",
        "        'Model': model_name,\n",
        "        'Rationale': rationale,\n",
        "        'Parameters': str(parameters),\n",
        "        'CV_Scores': cv_scores,\n",
        "        'Mean_MSE': mean_score,\n",
        "        'Std_MSE': std_score,\n",
        "        'Notes': additional_notes,\n",
        "        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "    experiment_results.append(result)\n",
        "    return result\n",
        "\n",
        "def show_experiment_summary():\n",
        "    \"\"\"\n",
        "    Display summary of all experiments\n",
        "    \"\"\"\n",
        "    df_results = pd.DataFrame(experiment_results)\n",
        "    df_summary = df_results[['Experiment_ID', 'Model', 'Mean_MSE', 'Std_MSE', 'Rationale']].copy()\n",
        "    df_summary = df_summary.sort_values('Mean_MSE')\n",
        "    return df_summary\n",
        "\n",
        "print(\"Experiment logging functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufPIaJLPc-3d"
      },
      "source": [
        "## Baseline: Current Best Model (XGBoost from main notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wuBeuDfyc-3d",
        "outputId": "da53f71e-745e-4a93-ac82-18be06363f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Baseline XGBoost...\n",
            "  Fold 1: MSE = 3707.64\n",
            "  Fold 2: MSE = 3692.62\n",
            "  Fold 3: MSE = 3676.71\n",
            "\n",
            "Baseline Mean MSE: 3692.32 ± 12.63\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Experiment_ID': 'BASELINE',\n",
              " 'Model': 'XGBoost (Original)',\n",
              " 'Rationale': 'Current best model from main notebook - establishes benchmark',\n",
              " 'Parameters': \"{'objective': 'multi:softprob', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'mlogloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.05, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 300, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': 1, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'hist', 'validate_parameters': None, 'verbosity': None, 'num_class': 5, 'use_label_encoder': False}\",\n",
              " 'CV_Scores': [3707.6387130776884, 3692.621461627855, 3676.711176936086],\n",
              " 'Mean_MSE': np.float64(3692.323783880543),\n",
              " 'Std_MSE': np.float64(12.62786817528244),\n",
              " 'Notes': 'Uses basic features with log1p transform on numerical features',\n",
              " 'Timestamp': '2025-11-23 07:32:17'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Prepare baseline data\n",
        "X_baseline = df_clean.drop(columns=['Delay_Category'])\n",
        "y_baseline = df_clean['Delay_Category']\n",
        "\n",
        "NUMERICAL_FEATURES = ['Distance', 'CRSDepTime']\n",
        "CATEGORICAL_FEATURES_BASE = X_baseline.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_baseline = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_BASE)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "xgb_baseline = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    eval_metric='mlogloss',\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.05,\n",
        "    num_class=NUM_CLASSES,\n",
        "    reg_lambda=1,\n",
        "    use_label_encoder=False,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "# Run baseline cross-validation\n",
        "print(\"Running Baseline XGBoost...\")\n",
        "cv_scores_baseline = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_baseline, y_baseline), 1):\n",
        "    X_train, X_test = X_baseline.iloc[train_index], X_baseline.iloc[test_index]\n",
        "    y_train, y_test = y_baseline.iloc[train_index], y_baseline.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_baseline.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_baseline.transform(X_test)\n",
        "\n",
        "    xgb_baseline.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = xgb_baseline.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_baseline.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_baseline = np.mean(cv_scores_baseline)\n",
        "std_mse_baseline = np.std(cv_scores_baseline)\n",
        "\n",
        "print(f\"\\nBaseline Mean MSE: {mean_mse_baseline:.2f} ± {std_mse_baseline:.2f}\")\n",
        "\n",
        "# Log baseline\n",
        "log_experiment(\n",
        "    experiment_id=\"BASELINE\",\n",
        "    model_name=\"XGBoost (Original)\",\n",
        "    rationale=\"Current best model from main notebook - establishes benchmark\",\n",
        "    parameters=xgb_baseline.get_params(),\n",
        "    cv_scores=cv_scores_baseline,\n",
        "    mean_score=mean_mse_baseline,\n",
        "    std_score=std_mse_baseline,\n",
        "    additional_notes=\"Uses basic features with log1p transform on numerical features\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8T1kU7pc-3d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sVGaxZRmc-3d",
        "outputId": "e6e420b0-7b1e-45e1-a17e-fe24e7104bdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting feature engineering with base shape: (582425, 13)\n"
          ]
        }
      ],
      "source": [
        "# Create a copy for feature engineering\n",
        "df_features = df_clean.copy()\n",
        "print(f\"Starting feature engineering with base shape: {df_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjWclf2pc-3d"
      },
      "source": [
        "## Experiment A1: Temporal Features - Weekend Indicator\n",
        "\n",
        "**Rationale**: Weekends may have different delay patterns (less business travel, different staffing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EqrAf2EOc-3d",
        "outputId": "d91dd989-6ff9-4cc4-f682-5ae15e564b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment A1: Weekend Indicator\n",
            "Mean MSE: 3695.18 ± 15.13\n",
            "Improvement vs Baseline: -2.85\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Experiment_ID': 'A1',\n",
              " 'Model': 'XGBoost + Weekend',\n",
              " 'Rationale': 'Weekend flights may have different delay patterns',\n",
              " 'Parameters': \"{'added_features': 'IsWeekend'}\",\n",
              " 'CV_Scores': [3714.7936848356267, 3692.758611460157, 3677.973480907032],\n",
              " 'Mean_MSE': np.float64(3695.1752590676056),\n",
              " 'Std_MSE': np.float64(15.128604112293695),\n",
              " 'Notes': 'Improvement: -2.85',\n",
              " 'Timestamp': '2025-11-23 07:35:50'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Add weekend indicator (DayOfWeek: 1=Monday, ..., 7=Sunday)\n",
        "df_features['IsWeekend'] = (df_features['DayOfWeek'].isin([6, 7])).astype('category')\n",
        "\n",
        "# Test with baseline XGBoost\n",
        "X_a1 = df_features.drop(columns=['Delay_Category'])\n",
        "y_a1 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_A1 = X_a1.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_a1 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_A1)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment A1: Weekend Indicator\")\n",
        "cv_scores_a1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_a1, y_a1), 1):\n",
        "    X_train, X_test = X_a1.iloc[train_index], X_a1.iloc[test_index]\n",
        "    y_train, y_test = y_a1.iloc[train_index], y_a1.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_a1.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_a1.transform(X_test)\n",
        "\n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_a1.append(custom_mse)\n",
        "\n",
        "mean_mse_a1 = np.mean(cv_scores_a1)\n",
        "std_mse_a1 = np.std(cv_scores_a1)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_a1:.2f} ± {std_mse_a1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_a1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"A1\",\n",
        "    model_name=\"XGBoost + Weekend\",\n",
        "    rationale=\"Weekend flights may have different delay patterns\",\n",
        "    parameters={\"added_features\": \"IsWeekend\"},\n",
        "    cv_scores=cv_scores_a1,\n",
        "    mean_score=mean_mse_a1,\n",
        "    std_score=std_mse_a1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_a1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM0ZQQzXc-3e"
      },
      "source": [
        "## Experiment A2: Temporal Features - Rush Hour Indicator\n",
        "\n",
        "**Rationale**: Morning (6-9am) and evening (4-7pm) rush hours may have more congestion and delays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fgpp5RPjc-3e",
        "outputId": "936aa0dc-8738-45a2-e5f5-75562c5b42fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment A2: Rush Hour Indicator\n",
            "Mean MSE: 3695.02 ± 13.36\n",
            "Improvement vs Baseline: -2.70\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Experiment_ID': 'A2',\n",
              " 'Model': 'XGBoost + Rush Hour',\n",
              " 'Rationale': 'Rush hour flights face more airport congestion',\n",
              " 'Parameters': \"{'added_features': 'IsWeekend, IsRushHour'}\",\n",
              " 'CV_Scores': [3711.60719918263, 3694.557443122182, 3678.896865454174],\n",
              " 'Mean_MSE': np.float64(3695.020502586329),\n",
              " 'Std_MSE': np.float64(13.357951469873555),\n",
              " 'Notes': 'Improvement: -2.70',\n",
              " 'Timestamp': '2025-11-23 07:39:27'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Add rush hour indicator\n",
        "def is_rush_hour(time_minutes):\n",
        "    hour = time_minutes // 60\n",
        "    # Morning rush: 6-9am, Evening rush: 4-7pm (16-19)\n",
        "    return ((6 <= hour < 9) | (16 <= hour < 19))\n",
        "\n",
        "df_features['IsRushHour'] = df_features['CRSDepTime'].apply(is_rush_hour).astype('category')\n",
        "\n",
        "X_a2 = df_features.drop(columns=['Delay_Category'])\n",
        "y_a2 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_A2 = X_a2.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_a2 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_A2)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment A2: Rush Hour Indicator\")\n",
        "cv_scores_a2 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_a2, y_a2), 1):\n",
        "    X_train, X_test = X_a2.iloc[train_index], X_a2.iloc[test_index]\n",
        "    y_train, y_test = y_a2.iloc[train_index], y_a2.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_a2.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_a2.transform(X_test)\n",
        "\n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_a2.append(custom_mse)\n",
        "\n",
        "mean_mse_a2 = np.mean(cv_scores_a2)\n",
        "std_mse_a2 = np.std(cv_scores_a2)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_a2:.2f} ± {std_mse_a2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_a2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"A2\",\n",
        "    model_name=\"XGBoost + Rush Hour\",\n",
        "    rationale=\"Rush hour flights face more airport congestion\",\n",
        "    parameters={\"added_features\": \"IsWeekend, IsRushHour\"},\n",
        "    cv_scores=cv_scores_a2,\n",
        "    mean_score=mean_mse_a2,\n",
        "    std_score=std_mse_a2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_a2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0kovLvtc-3e"
      },
      "source": [
        "## Experiment B1: Route Features - Distance Bins\n",
        "\n",
        "**Rationale**: Short, medium, and long-haul flights have different operational characteristics and delay patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6BFEotvYc-3e",
        "outputId": "e66f5848-938c-4e69-d36b-36474a4820c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment B1: Distance Bins\n",
            "Mean MSE: 3694.44 ± 13.45\n",
            "Improvement vs Baseline: -2.11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Experiment_ID': 'B1',\n",
              " 'Model': 'XGBoost + Distance Bins',\n",
              " 'Rationale': 'Different flight distances have different delay characteristics',\n",
              " 'Parameters': \"{'added_features': 'IsWeekend, IsRushHour, Distance_Bin'}\",\n",
              " 'CV_Scores': [3712.536462336581, 3690.470433776549, 3680.305161783042],\n",
              " 'Mean_MSE': np.float64(3694.4373526320574),\n",
              " 'Std_MSE': np.float64(13.454033292357092),\n",
              " 'Notes': 'Improvement: -2.11',\n",
              " 'Timestamp': '2025-11-23 07:43:08'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Add distance bins\n",
        "def categorize_distance(distance):\n",
        "    if distance < 500:\n",
        "        return 'Short'  # <500 miles\n",
        "    elif distance < 1500:\n",
        "        return 'Medium'  # 500-1500 miles\n",
        "    else:\n",
        "        return 'Long'  # >1500 miles\n",
        "\n",
        "df_features['Distance_Bin'] = df_features['Distance'].apply(categorize_distance).astype('category')\n",
        "\n",
        "X_b1 = df_features.drop(columns=['Delay_Category'])\n",
        "y_b1 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_B1 = X_b1.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_b1 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_B1)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment B1: Distance Bins\")\n",
        "cv_scores_b1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_b1, y_b1), 1):\n",
        "    X_train, X_test = X_b1.iloc[train_index], X_b1.iloc[test_index]\n",
        "    y_train, y_test = y_b1.iloc[train_index], y_b1.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_b1.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_b1.transform(X_test)\n",
        "\n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_b1.append(custom_mse)\n",
        "\n",
        "mean_mse_b1 = np.mean(cv_scores_b1)\n",
        "std_mse_b1 = np.std(cv_scores_b1)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_b1:.2f} ± {std_mse_b1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_b1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"B1\",\n",
        "    model_name=\"XGBoost + Distance Bins\",\n",
        "    rationale=\"Different flight distances have different delay characteristics\",\n",
        "    parameters={\"added_features\": \"IsWeekend, IsRushHour, Distance_Bin\"},\n",
        "    cv_scores=cv_scores_b1,\n",
        "    mean_score=mean_mse_b1,\n",
        "    std_score=std_mse_b1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_b1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb3TZjxpc-3e"
      },
      "source": [
        "## Experiment C1: Historical Features - Airport Delay Rates\n",
        "\n",
        "**Rationale**: Some airports have consistently higher delay rates; calculate aggregated statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YQjGKxMQc-3e",
        "outputId": "4b11c43e-3dc3-41f5-d364-51f677dec0e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment C1: Airport Delay Rates\n",
            "Mean MSE: 3632.48 ± 11.33\n",
            "Improvement vs Baseline: 59.84\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Experiment_ID': 'C1',\n",
              " 'Model': 'XGBoost + Airport Delay Rates',\n",
              " 'Rationale': 'Historical airport performance predicts future delays',\n",
              " 'Parameters': \"{'added_features': 'All previous + Origin_Delay_Rate, Dest_Delay_Rate'}\",\n",
              " 'CV_Scores': [3646.083764439104, 3633.013038964137, 3618.3494787803006],\n",
              " 'Mean_MSE': np.float64(3632.4820940611808),\n",
              " 'Std_MSE': np.float64(11.328697398451087),\n",
              " 'Notes': 'Improvement: 59.84',\n",
              " 'Timestamp': '2025-11-23 07:46:59'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate origin and destination airport delay rates\n",
        "# Use the full dataset to avoid data leakage (we're using historical aggregate stats)\n",
        "origin_delay_rate = df_features.groupby('OriginAirportID')['Delay_Category'].apply(lambda x: (x > 0).mean())\n",
        "dest_delay_rate = df_features.groupby('DestAirportID')['Delay_Category'].apply(lambda x: (x > 0).mean())\n",
        "\n",
        "df_features['Origin_Delay_Rate'] = df_features['OriginAirportID'].map(origin_delay_rate)\n",
        "df_features['Dest_Delay_Rate'] = df_features['DestAirportID'].map(dest_delay_rate)\n",
        "\n",
        "# Update numerical features\n",
        "NUMERICAL_FEATURES_C1 = ['Distance', 'CRSDepTime', 'Origin_Delay_Rate', 'Dest_Delay_Rate']\n",
        "\n",
        "X_c1 = df_features.drop(columns=['Delay_Category'])\n",
        "y_c1 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_C1 = X_c1.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_c1 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES_C1),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_C1)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment C1: Airport Delay Rates\")\n",
        "cv_scores_c1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_c1, y_c1), 1):\n",
        "    X_train, X_test = X_c1.iloc[train_index], X_c1.iloc[test_index]\n",
        "    y_train, y_test = y_c1.iloc[train_index], y_c1.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_c1.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_c1.transform(X_test)\n",
        "\n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_c1.append(custom_mse)\n",
        "\n",
        "mean_mse_c1 = np.mean(cv_scores_c1)\n",
        "std_mse_c1 = np.std(cv_scores_c1)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_c1:.2f} ± {std_mse_c1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_c1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"C1\",\n",
        "    model_name=\"XGBoost + Airport Delay Rates\",\n",
        "    rationale=\"Historical airport performance predicts future delays\",\n",
        "    parameters={\"added_features\": \"All previous + Origin_Delay_Rate, Dest_Delay_Rate\"},\n",
        "    cv_scores=cv_scores_c1,\n",
        "    mean_score=mean_mse_c1,\n",
        "    std_score=std_mse_c1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_c1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IMm0auMc-3e"
      },
      "source": [
        "## Experiment C2: Historical Features - Airline Delay Rates\n",
        "\n",
        "**Rationale**: Different airlines have different operational efficiency and delay rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V5frRFz7c-3e",
        "outputId": "4c41e3bc-1266-4f86-df11-bded2f1fa264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment C2: Airline Delay Rates\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Object with dtype category cannot perform the numpy op log1p",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-893216463.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_c2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_c2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mX_train_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor_c2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mX_test_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor_c2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_empty_routing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         result = self._call_func_on_transformers(\n\u001b[0m\u001b[1;32m   1002\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_call_func_on_transformers\u001b[0;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[1;32m    908\u001b[0m                 )\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \"\"\"\n\u001b[1;32m    717\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_function_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \"\"\"\n\u001b[1;32m    259\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkw_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0moutput_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dense\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_function_transformer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, func, kw_args)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_identity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkw_args\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkw_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_is_fitted__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-893216463.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     transformers=[\n\u001b[1;32m     14\u001b[0m         ('num', Pipeline([\n\u001b[0;32m---> 15\u001b[0;31m              \u001b[0;34m(\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m              ('scaler', StandardScaler())]), NUMERICAL_FEATURES_C2),\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCATEGORICAL_FEATURES_C2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m     ):\n\u001b[0;32m-> 2171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marraylike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_ufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# take this path if there are no kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# otherwise specific ufunc methods (eg np.<ufunc>.accumulate(..))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0;31m# for all other cases, raise for now (similarly as what happens in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;31m# Series.__array_prepare__)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;34mf\"Object with dtype {self.dtype} cannot perform \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0;34mf\"the numpy op {ufunc.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object with dtype category cannot perform the numpy op log1p"
          ]
        }
      ],
      "source": [
        "# Calculate airline delay rates\n",
        "airline_delay_rate = df_features.groupby('Marketing_Airline_Network')['Delay_Category'].apply(lambda x: (x > 0).mean())\n",
        "df_features['Airline_Delay_Rate'] = df_features['Marketing_Airline_Network'].map(airline_delay_rate)\n",
        "\n",
        "NUMERICAL_FEATURES_C2 = ['Distance', 'CRSDepTime', 'Origin_Delay_Rate', 'Dest_Delay_Rate', 'Airline_Delay_Rate']\n",
        "\n",
        "X_c2 = df_features.drop(columns=['Delay_Category'])\n",
        "y_c2 = df_features['Delay_Category']\n",
        "\n",
        "CATEGORICAL_FEATURES_C2 = X_c2.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "preprocessor_c2 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES_C2),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_C2)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"Experiment C2: Airline Delay Rates\")\n",
        "cv_scores_c2 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_c2, y_c2), 1):\n",
        "    X_train, X_test = X_c2.iloc[train_index], X_c2.iloc[test_index]\n",
        "    y_train, y_test = y_c2.iloc[train_index], y_c2.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_c2.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_c2.transform(X_test)\n",
        "\n",
        "    model = XGBClassifier(**xgb_baseline.get_params())\n",
        "    model.fit(X_train_processed, y_train, verbose=False)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_c2.append(custom_mse)\n",
        "\n",
        "mean_mse_c2 = np.mean(cv_scores_c2)\n",
        "std_mse_c2 = np.std(cv_scores_c2)\n",
        "\n",
        "print(f\"Mean MSE: {mean_mse_c2:.2f} ± {std_mse_c2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_c2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"C2\",\n",
        "    model_name=\"XGBoost + Airline Delay Rates\",\n",
        "    rationale=\"Airline operational efficiency is a strong predictor\",\n",
        "    parameters={\"added_features\": \"All previous + Airline_Delay_Rate\"},\n",
        "    cv_scores=cv_scores_c2,\n",
        "    mean_score=mean_mse_c2,\n",
        "    std_score=std_mse_c2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_c2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzaIVMj9c-3e"
      },
      "source": [
        "## Feature Engineering Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onCRKkVgc-3e"
      },
      "outputs": [],
      "source": [
        "# Show feature engineering results\n",
        "print(\"=\"*70)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "feature_summary = show_experiment_summary()\n",
        "print(feature_summary.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Determine best feature set\n",
        "best_features_exp = feature_summary.iloc[0]\n",
        "print(f\"\\nBest Feature Set: {best_features_exp['Experiment_ID']} - {best_features_exp['Model']}\")\n",
        "print(f\"Best MSE: {best_features_exp['Mean_MSE']:.2f}\")\n",
        "print(f\"Improvement over baseline: {mean_mse_baseline - best_features_exp['Mean_MSE']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI6Afd-yc-3e"
      },
      "source": [
        "---\n",
        "# Phase 3: Model Architecture Exploration\n",
        "\n",
        "**Strategy**: Test different model families with the best feature set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpDz02Pwc-3e"
      },
      "outputs": [],
      "source": [
        "# Use the best feature set from previous phase\n",
        "# For now, we'll use all engineered features (C2)\n",
        "X_best = df_features.drop(columns=['Delay_Category'])\n",
        "y_best = df_features['Delay_Category']\n",
        "\n",
        "NUMERICAL_FEATURES_BEST = ['Distance', 'CRSDepTime', 'Origin_Delay_Rate', 'Dest_Delay_Rate', 'Airline_Delay_Rate']\n",
        "CATEGORICAL_FEATURES_BEST = X_best.select_dtypes(include='category').columns.tolist()\n",
        "\n",
        "print(f\"Best feature set: {len(NUMERICAL_FEATURES_BEST)} numerical + {len(CATEGORICAL_FEATURES_BEST)} categorical\")\n",
        "print(f\"Total samples: {len(X_best)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwkgkOG7c-3f"
      },
      "source": [
        "## Experiment D1: LightGBM Model\n",
        "\n",
        "**Rationale**: LightGBM is faster than XGBoost and often performs better with categorical features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQiEG1WHc-3f"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "preprocessor_best = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "             ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "             ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_BEST)\n",
        "    ], remainder='drop'\n",
        ")\n",
        "\n",
        "lgbm_model = LGBMClassifier(\n",
        "    objective='multiclass',\n",
        "    num_class=NUM_CLASSES,\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.05,\n",
        "    reg_lambda=1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "print(\"Experiment D1: LightGBM\")\n",
        "cv_scores_d1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "\n",
        "    lgbm_model.fit(X_train_processed, y_train)\n",
        "    y_pred_proba = lgbm_model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_d1.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_d1 = np.mean(cv_scores_d1)\n",
        "std_mse_d1 = np.std(cv_scores_d1)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_d1:.2f} ± {std_mse_d1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_d1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"D1\",\n",
        "    model_name=\"LightGBM\",\n",
        "    rationale=\"LightGBM often outperforms XGBoost with better categorical handling\",\n",
        "    parameters=lgbm_model.get_params(),\n",
        "    cv_scores=cv_scores_d1,\n",
        "    mean_score=mean_mse_d1,\n",
        "    std_score=std_mse_d1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_d1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkF8y272c-3f"
      },
      "source": [
        "## Experiment D2: CatBoost Model\n",
        "\n",
        "**Rationale**: CatBoost has native categorical feature support and strong baseline performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2WXt5w-c-3f"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost can handle categorical features natively\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=300,\n",
        "    depth=7,\n",
        "    learning_rate=0.05,\n",
        "    loss_function='MultiClass',\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=False,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "print(\"Experiment D2: CatBoost\")\n",
        "cv_scores_d2 = []\n",
        "\n",
        "# Get categorical feature indices for CatBoost\n",
        "cat_feature_indices = [i for i, col in enumerate(X_best.columns) if col in CATEGORICAL_FEATURES_BEST]\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "    # CatBoost preprocessor (only numerical features need transformation)\n",
        "    preprocessor_catboost = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                 ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                 ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "        ], remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    X_train_processed = preprocessor_catboost.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_catboost.transform(X_test)\n",
        "\n",
        "    # Identify new categorical indices after preprocessing\n",
        "    num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "    cat_indices_processed = list(range(num_numerical, X_train_processed.shape[1]))\n",
        "\n",
        "    catboost_model.fit(X_train_processed, y_train, cat_features=cat_indices_processed)\n",
        "    y_pred_proba = catboost_model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_d2.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_d2 = np.mean(cv_scores_d2)\n",
        "std_mse_d2 = np.std(cv_scores_d2)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_d2:.2f} ± {std_mse_d2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_d2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"D2\",\n",
        "    model_name=\"CatBoost\",\n",
        "    rationale=\"Native categorical handling may improve performance\",\n",
        "    parameters=catboost_model.get_params(),\n",
        "    cv_scores=cv_scores_d2,\n",
        "    mean_score=mean_mse_d2,\n",
        "    std_score=std_mse_d2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_d2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWsnV8Npc-3f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3VLx5-Hc-3f"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Experiment D3: Random Forest\")\n",
        "cv_scores_d3 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "\n",
        "    rf_model.fit(X_train_processed, y_train)\n",
        "    y_pred_proba = rf_model.predict_proba(X_test_processed)\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "    cv_scores_d3.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_d3 = np.mean(cv_scores_d3)\n",
        "std_mse_d3 = np.std(cv_scores_d3)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_d3:.2f} ± {std_mse_d3:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_d3:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"D3\",\n",
        "    model_name=\"Random Forest\",\n",
        "    rationale=\"Non-boosted ensemble as baseline comparison\",\n",
        "    parameters=rf_model.get_params(),\n",
        "    cv_scores=cv_scores_d3,\n",
        "    mean_score=mean_mse_d3,\n",
        "    std_score=std_mse_d3,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_d3:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JRXx64Lc-3f"
      },
      "source": [
        "## Model Architecture Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnjRrnzYc-3f"
      },
      "outputs": [],
      "source": [
        "# Show all experiment results so far\n",
        "print(\"=\"*70)\n",
        "print(\"ALL EXPERIMENTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "all_summary = show_experiment_summary()\n",
        "print(all_summary.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Identify top 3 models for hyperparameter tuning\n",
        "top3_models = all_summary.head(3)\n",
        "print(f\"\\nTop 3 Models for Hyperparameter Tuning:\")\n",
        "for idx, row in top3_models.iterrows():\n",
        "    print(f\"{row['Experiment_ID']}: {row['Model']} - MSE: {row['Mean_MSE']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEB7MNP2c-3f"
      },
      "source": [
        "---\n",
        "# Phase 4: Hyperparameter Optimization\n",
        "\n",
        "**Strategy**: Tune the top-performing models systematically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySqMgaQc-3f"
      },
      "source": [
        "## Experiment E1: XGBoost Hyperparameter Tuning\n",
        "\n",
        "**Rationale**: Systematically search for optimal learning rate, depth, and regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft1wU5jOc-3f"
      },
      "outputs": [],
      "source": [
        "# XGBoost hyperparameter grid search\n",
        "print(\"Experiment E1: XGBoost Hyperparameter Tuning\")\n",
        "\n",
        "param_grid_xgb = [\n",
        "    # Test 1: Deeper trees with more regularization\n",
        "    {'max_depth': 10, 'learning_rate': 0.03, 'n_estimators': 400, 'reg_lambda': 2, 'subsample': 0.8},\n",
        "    # Test 2: Shallower trees with higher learning rate\n",
        "    {'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 200, 'reg_lambda': 1, 'subsample': 0.9},\n",
        "    # Test 3: Balanced approach\n",
        "    {'max_depth': 8, 'learning_rate': 0.05, 'n_estimators': 350, 'reg_lambda': 1.5, 'subsample': 0.85},\n",
        "    # Test 4: More estimators with lower learning rate\n",
        "    {'max_depth': 7, 'learning_rate': 0.02, 'n_estimators': 500, 'reg_lambda': 1, 'subsample': 0.9},\n",
        "]\n",
        "\n",
        "best_xgb_score = float('inf')\n",
        "best_xgb_params = None\n",
        "\n",
        "for idx, params in enumerate(param_grid_xgb, 1):\n",
        "    print(f\"\\nTesting XGBoost config {idx}/{len(param_grid_xgb)}: {params}\")\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        eval_metric='mlogloss',\n",
        "        num_class=NUM_CLASSES,\n",
        "        use_label_encoder=False,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist',\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    cv_scores = []\n",
        "    for train_index, test_index in skf.split(X_best, y_best):\n",
        "        X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "        y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "        X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "        X_test_processed = preprocessor_best.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_processed, y_train, verbose=False)\n",
        "        y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "        custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "        cv_scores.append(custom_mse)\n",
        "\n",
        "    mean_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    print(f\"  Mean MSE: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "\n",
        "    log_experiment(\n",
        "        experiment_id=f\"E1_{idx}\",\n",
        "        model_name=f\"XGBoost Tuned {idx}\",\n",
        "        rationale=f\"Hyperparameter config {idx}\",\n",
        "        parameters=params,\n",
        "        cv_scores=cv_scores,\n",
        "        mean_score=mean_score,\n",
        "        std_score=std_score\n",
        "    )\n",
        "\n",
        "    if mean_score < best_xgb_score:\n",
        "        best_xgb_score = mean_score\n",
        "        best_xgb_params = params\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best XGBoost Config: {best_xgb_params}\")\n",
        "print(f\"Best XGBoost MSE: {best_xgb_score:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - best_xgb_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8UZg8D4c-3f"
      },
      "source": [
        "## Experiment E2: LightGBM Hyperparameter Tuning\n",
        "\n",
        "**Rationale**: Optimize LightGBM-specific parameters (num_leaves, min_child_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRZ8AA9Ic-3f"
      },
      "outputs": [],
      "source": [
        "# LightGBM hyperparameter grid search\n",
        "print(\"Experiment E2: LightGBM Hyperparameter Tuning\")\n",
        "\n",
        "param_grid_lgbm = [\n",
        "    # Test 1: More leaves with regularization\n",
        "    {'num_leaves': 127, 'learning_rate': 0.03, 'n_estimators': 400, 'reg_lambda': 2, 'min_child_samples': 30},\n",
        "    # Test 2: Fewer leaves, higher learning rate\n",
        "    {'num_leaves': 31, 'learning_rate': 0.1, 'n_estimators': 200, 'reg_lambda': 1, 'min_child_samples': 20},\n",
        "    # Test 3: Balanced approach\n",
        "    {'num_leaves': 63, 'learning_rate': 0.05, 'n_estimators': 350, 'reg_lambda': 1.5, 'min_child_samples': 25},\n",
        "    # Test 4: Conservative with more estimators\n",
        "    {'num_leaves': 50, 'learning_rate': 0.02, 'n_estimators': 500, 'reg_lambda': 1, 'min_child_samples': 20},\n",
        "]\n",
        "\n",
        "best_lgbm_score = float('inf')\n",
        "best_lgbm_params = None\n",
        "\n",
        "for idx, params in enumerate(param_grid_lgbm, 1):\n",
        "    print(f\"\\nTesting LightGBM config {idx}/{len(param_grid_lgbm)}: {params}\")\n",
        "\n",
        "    model = LGBMClassifier(\n",
        "        objective='multiclass',\n",
        "        num_class=NUM_CLASSES,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    cv_scores = []\n",
        "    for train_index, test_index in skf.split(X_best, y_best):\n",
        "        X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "        y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "        X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "        X_test_processed = preprocessor_best.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_processed, y_train)\n",
        "        y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "        custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "        cv_scores.append(custom_mse)\n",
        "\n",
        "    mean_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    print(f\"  Mean MSE: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "\n",
        "    log_experiment(\n",
        "        experiment_id=f\"E2_{idx}\",\n",
        "        model_name=f\"LightGBM Tuned {idx}\",\n",
        "        rationale=f\"Hyperparameter config {idx}\",\n",
        "        parameters=params,\n",
        "        cv_scores=cv_scores,\n",
        "        mean_score=mean_score,\n",
        "        std_score=std_score\n",
        "    )\n",
        "\n",
        "    if mean_score < best_lgbm_score:\n",
        "        best_lgbm_score = mean_score\n",
        "        best_lgbm_params = params\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best LightGBM Config: {best_lgbm_params}\")\n",
        "print(f\"Best LightGBM MSE: {best_lgbm_score:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - best_lgbm_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfVMENSc-3k"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPvU8-upc-3k"
      },
      "outputs": [],
      "source": [
        "# CatBoost hyperparameter grid search\n",
        "print(\"Experiment E3: CatBoost Hyperparameter Tuning\")\n",
        "\n",
        "param_grid_catboost = [\n",
        "    # Test 1: Deeper trees\n",
        "    {'depth': 10, 'learning_rate': 0.03, 'iterations': 400, 'l2_leaf_reg': 5},\n",
        "    # Test 2: Shallower trees, higher LR\n",
        "    {'depth': 6, 'learning_rate': 0.1, 'iterations': 200, 'l2_leaf_reg': 3},\n",
        "    # Test 3: Balanced\n",
        "    {'depth': 8, 'learning_rate': 0.05, 'iterations': 350, 'l2_leaf_reg': 4},\n",
        "    # Test 4: More iterations\n",
        "    {'depth': 7, 'learning_rate': 0.02, 'iterations': 500, 'l2_leaf_reg': 3},\n",
        "]\n",
        "\n",
        "best_catboost_score = float('inf')\n",
        "best_catboost_params = None\n",
        "\n",
        "for idx, params in enumerate(param_grid_catboost, 1):\n",
        "    print(f\"\\nTesting CatBoost config {idx}/{len(param_grid_catboost)}: {params}\")\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        loss_function='MultiClass',\n",
        "        random_state=RANDOM_STATE,\n",
        "        verbose=False,\n",
        "        thread_count=-1,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    cv_scores = []\n",
        "    for train_index, test_index in skf.split(X_best, y_best):\n",
        "        X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "        y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "        preprocessor_catboost = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', Pipeline([\n",
        "                     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                     ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "            ], remainder='passthrough'\n",
        "        )\n",
        "\n",
        "        X_train_processed = preprocessor_catboost.fit_transform(X_train)\n",
        "        X_test_processed = preprocessor_catboost.transform(X_test)\n",
        "\n",
        "        num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "        cat_indices = list(range(num_numerical, X_train_processed.shape[1]))\n",
        "\n",
        "        model.fit(X_train_processed, y_train, cat_features=cat_indices)\n",
        "        y_pred_proba = model.predict_proba(X_test_processed)\n",
        "\n",
        "        custom_mse, _, _ = calculate_expected_payout_mse(y_test, y_pred_proba)\n",
        "        cv_scores.append(custom_mse)\n",
        "\n",
        "    mean_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    print(f\"  Mean MSE: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "\n",
        "    log_experiment(\n",
        "        experiment_id=f\"E3_{idx}\",\n",
        "        model_name=f\"CatBoost Tuned {idx}\",\n",
        "        rationale=f\"Hyperparameter config {idx}\",\n",
        "        parameters=params,\n",
        "        cv_scores=cv_scores,\n",
        "        mean_score=mean_score,\n",
        "        std_score=std_score\n",
        "    )\n",
        "\n",
        "    if mean_score < best_catboost_score:\n",
        "        best_catboost_score = mean_score\n",
        "        best_catboost_params = params\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best CatBoost Config: {best_catboost_params}\")\n",
        "print(f\"Best CatBoost MSE: {best_catboost_score:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - best_catboost_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mqn8-kWc-3k"
      },
      "source": [
        "---\n",
        "# Phase 5: Ensemble Methods\n",
        "\n",
        "**Strategy**: Combine predictions from multiple models to reduce variance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9jtqojFc-3k"
      },
      "source": [
        "## Experiment F1: Simple Averaging Ensemble\n",
        "\n",
        "**Rationale**: Average predictions from top 3 models to reduce variance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLGTSvZwc-3k"
      },
      "outputs": [],
      "source": [
        "print(\"Experiment F1: Simple Averaging Ensemble\")\n",
        "\n",
        "# Create best models based on hyperparameter tuning\n",
        "best_xgb = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    eval_metric='mlogloss',\n",
        "    num_class=NUM_CLASSES,\n",
        "    use_label_encoder=False,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    tree_method='hist',\n",
        "    **best_xgb_params\n",
        ")\n",
        "\n",
        "best_lgbm = LGBMClassifier(\n",
        "    objective='multiclass',\n",
        "    num_class=NUM_CLASSES,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1,\n",
        "    **best_lgbm_params\n",
        ")\n",
        "\n",
        "best_catboost = CatBoostClassifier(\n",
        "    loss_function='MultiClass',\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=False,\n",
        "    thread_count=-1,\n",
        "    **best_catboost_params\n",
        ")\n",
        "\n",
        "cv_scores_f1 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "    # XGBoost and LightGBM\n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "\n",
        "    best_xgb.fit(X_train_processed, y_train, verbose=False)\n",
        "    xgb_proba = best_xgb.predict_proba(X_test_processed)\n",
        "\n",
        "    best_lgbm.fit(X_train_processed, y_train)\n",
        "    lgbm_proba = best_lgbm.predict_proba(X_test_processed)\n",
        "\n",
        "    # CatBoost\n",
        "    preprocessor_catboost = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                 ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                 ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "        ], remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    X_train_cat = preprocessor_catboost.fit_transform(X_train)\n",
        "    X_test_cat = preprocessor_catboost.transform(X_test)\n",
        "\n",
        "    num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "    cat_indices = list(range(num_numerical, X_train_cat.shape[1]))\n",
        "\n",
        "    best_catboost.fit(X_train_cat, y_train, cat_features=cat_indices)\n",
        "    catboost_proba = best_catboost.predict_proba(X_test_cat)\n",
        "\n",
        "    # Simple average ensemble\n",
        "    ensemble_proba = (xgb_proba + lgbm_proba + catboost_proba) / 3\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, ensemble_proba)\n",
        "    cv_scores_f1.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_f1 = np.mean(cv_scores_f1)\n",
        "std_mse_f1 = np.std(cv_scores_f1)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_f1:.2f} ± {std_mse_f1:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_f1:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"F1\",\n",
        "    model_name=\"Ensemble (XGB+LGBM+CAT avg)\",\n",
        "    rationale=\"Average of top 3 tuned models\",\n",
        "    parameters={\"models\": [\"XGBoost\", \"LightGBM\", \"CatBoost\"], \"method\": \"simple_average\"},\n",
        "    cv_scores=cv_scores_f1,\n",
        "    mean_score=mean_mse_f1,\n",
        "    std_score=std_mse_f1,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_f1:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbJot1_Qc-3k"
      },
      "source": [
        "## Experiment F2: Weighted Ensemble\n",
        "\n",
        "**Rationale**: Weight models based on their individual performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci0PEg6Sc-3k"
      },
      "outputs": [],
      "source": [
        "print(\"Experiment F2: Weighted Ensemble\")\n",
        "\n",
        "# Calculate weights inversely proportional to MSE\n",
        "total_error = best_xgb_score + best_lgbm_score + best_catboost_score\n",
        "w_xgb = (1/best_xgb_score) / ((1/best_xgb_score) + (1/best_lgbm_score) + (1/best_catboost_score))\n",
        "w_lgbm = (1/best_lgbm_score) / ((1/best_xgb_score) + (1/best_lgbm_score) + (1/best_catboost_score))\n",
        "w_catboost = (1/best_catboost_score) / ((1/best_xgb_score) + (1/best_lgbm_score) + (1/best_catboost_score))\n",
        "\n",
        "print(f\"Weights: XGB={w_xgb:.3f}, LGBM={w_lgbm:.3f}, CAT={w_catboost:.3f}\")\n",
        "\n",
        "cv_scores_f2 = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_best, y_best), 1):\n",
        "    X_train, X_test = X_best.iloc[train_index], X_best.iloc[test_index]\n",
        "    y_train, y_test = y_best.iloc[train_index], y_best.iloc[test_index]\n",
        "\n",
        "    # Get predictions from all three models (same as F1)\n",
        "    X_train_processed = preprocessor_best.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor_best.transform(X_test)\n",
        "\n",
        "    best_xgb.fit(X_train_processed, y_train, verbose=False)\n",
        "    xgb_proba = best_xgb.predict_proba(X_test_processed)\n",
        "\n",
        "    best_lgbm.fit(X_train_processed, y_train)\n",
        "    lgbm_proba = best_lgbm.predict_proba(X_test_processed)\n",
        "\n",
        "    preprocessor_catboost = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                 ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
        "                 ('scaler', StandardScaler())]), NUMERICAL_FEATURES_BEST)\n",
        "        ], remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    X_train_cat = preprocessor_catboost.fit_transform(X_train)\n",
        "    X_test_cat = preprocessor_catboost.transform(X_test)\n",
        "\n",
        "    num_numerical = len(NUMERICAL_FEATURES_BEST)\n",
        "    cat_indices = list(range(num_numerical, X_train_cat.shape[1]))\n",
        "\n",
        "    best_catboost.fit(X_train_cat, y_train, cat_features=cat_indices)\n",
        "    catboost_proba = best_catboost.predict_proba(X_test_cat)\n",
        "\n",
        "    # Weighted ensemble\n",
        "    ensemble_proba = w_xgb * xgb_proba + w_lgbm * lgbm_proba + w_catboost * catboost_proba\n",
        "\n",
        "    custom_mse, _, _ = calculate_expected_payout_mse(y_test, ensemble_proba)\n",
        "    cv_scores_f2.append(custom_mse)\n",
        "    print(f\"  Fold {fold_idx}: MSE = {custom_mse:.2f}\")\n",
        "\n",
        "mean_mse_f2 = np.mean(cv_scores_f2)\n",
        "std_mse_f2 = np.std(cv_scores_f2)\n",
        "\n",
        "print(f\"\\nMean MSE: {mean_mse_f2:.2f} ± {std_mse_f2:.2f}\")\n",
        "print(f\"Improvement vs Baseline: {mean_mse_baseline - mean_mse_f2:.2f}\")\n",
        "\n",
        "log_experiment(\n",
        "    experiment_id=\"F2\",\n",
        "    model_name=\"Ensemble (Weighted)\",\n",
        "    rationale=\"Performance-weighted average of top 3 models\",\n",
        "    parameters={\"models\": [\"XGBoost\", \"LightGBM\", \"CatBoost\"], \"method\": \"weighted\",\n",
        "                \"weights\": f\"XGB={w_xgb:.3f}, LGBM={w_lgbm:.3f}, CAT={w_catboost:.3f}\"},\n",
        "    cv_scores=cv_scores_f2,\n",
        "    mean_score=mean_mse_f2,\n",
        "    std_score=std_mse_f2,\n",
        "    additional_notes=f\"Improvement: {mean_mse_baseline - mean_mse_f2:.2f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMTN5A8Gc-3l"
      },
      "source": [
        "---\n",
        "# Phase 6: Final Model Selection\n",
        "\n",
        "**Strategy**: Select the best performing model and document complete pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6NBK-rCc-3l"
      },
      "outputs": [],
      "source": [
        "# Final comprehensive summary\n",
        "print(\"=\"*80)\n",
        "print(\" \"*25 + \"FINAL EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "final_summary = show_experiment_summary()\n",
        "print(final_summary.head(10).to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select best model\n",
        "best_model_row = final_summary.iloc[0]\n",
        "print(f\"\\n🏆 BEST MODEL: {best_model_row['Model']}\")\n",
        "print(f\"   Experiment ID: {best_model_row['Experiment_ID']}\")\n",
        "print(f\"   Mean MSE: {best_model_row['Mean_MSE']:.2f} ± {best_model_row['Std_MSE']:.2f}\")\n",
        "print(f\"   Improvement over Baseline: {mean_mse_baseline - best_model_row['Mean_MSE']:.2f}\")\n",
        "print(f\"   Percentage Improvement: {((mean_mse_baseline - best_model_row['Mean_MSE'])/mean_mse_baseline * 100):.1f}%\")\n",
        "print(f\"\\nRationale: {best_model_row['Rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1iTANNJc-3l"
      },
      "source": [
        "## Model Performance Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4enZCnRc-3l"
      },
      "outputs": [],
      "source": [
        "# Visualize top 10 models\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "top_models = final_summary.head(10)\n",
        "y_pos = np.arange(len(top_models))\n",
        "\n",
        "bars = ax.barh(y_pos, top_models['Mean_MSE'], xerr=top_models['Std_MSE'],\n",
        "               color=['green' if i == 0 else 'steelblue' for i in range(len(top_models))])\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(top_models['Experiment_ID'] + ': ' + top_models['Model'])\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Payout MSE (lower is better)')\n",
        "ax.set_title('Top 10 Model Performances')\n",
        "ax.axvline(x=mean_mse_baseline, color='red', linestyle='--', label=f'Baseline: {mean_mse_baseline:.2f}')\n",
        "ax.legend()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg0JBcLyc-3l"
      },
      "source": [
        "## Key Findings and Insights\n",
        "\n",
        "Based on the experiments:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_Lq302Jc-3l"
      },
      "outputs": [],
      "source": [
        "print(\"KEY FINDINGS:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Feature engineering insights\n",
        "feature_exps = [exp for exp in experiment_results if exp['Experiment_ID'] in ['A1', 'A2', 'B1', 'C1', 'C2']]\n",
        "if feature_exps:\n",
        "    best_feature = min(feature_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n1. FEATURE ENGINEERING:\")\n",
        "    print(f\"   Most impactful features: {best_feature['Experiment_ID']} - {best_feature['Model']}\")\n",
        "    print(f\"   Improvement: {mean_mse_baseline - best_feature['Mean_MSE']:.2f}\")\n",
        "\n",
        "# Model architecture insights\n",
        "model_exps = [exp for exp in experiment_results if exp['Experiment_ID'] in ['D1', 'D2', 'D3']]\n",
        "if model_exps:\n",
        "    best_arch = min(model_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n2. MODEL ARCHITECTURE:\")\n",
        "    print(f\"   Best base model: {best_arch['Model']}\")\n",
        "    print(f\"   MSE: {best_arch['Mean_MSE']:.2f}\")\n",
        "\n",
        "# Hyperparameter tuning insights\n",
        "tuned_exps = [exp for exp in experiment_results if exp['Experiment_ID'].startswith('E')]\n",
        "if tuned_exps:\n",
        "    best_tuned = min(tuned_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n3. HYPERPARAMETER TUNING:\")\n",
        "    print(f\"   Best tuned model: {best_tuned['Model']}\")\n",
        "    print(f\"   MSE: {best_tuned['Mean_MSE']:.2f}\")\n",
        "\n",
        "# Ensemble insights\n",
        "ensemble_exps = [exp for exp in experiment_results if exp['Experiment_ID'] in ['F1', 'F2']]\n",
        "if ensemble_exps:\n",
        "    best_ensemble = min(ensemble_exps, key=lambda x: x['Mean_MSE'])\n",
        "    print(f\"\\n4. ENSEMBLE METHODS:\")\n",
        "    print(f\"   Best ensemble: {best_ensemble['Model']}\")\n",
        "    print(f\"   MSE: {best_ensemble['Mean_MSE']:.2f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"OVERALL BEST: {best_model_row['Model']}\")\n",
        "print(f\"Final MSE: {best_model_row['Mean_MSE']:.2f} (vs Baseline: {mean_mse_baseline:.2f})\")\n",
        "print(f\"Absolute Improvement: {mean_mse_baseline - best_model_row['Mean_MSE']:.2f}\")\n",
        "print(f\"Relative Improvement: {((mean_mse_baseline - best_model_row['Mean_MSE'])/mean_mse_baseline * 100):.1f}%\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqh_B00Dc-3l"
      },
      "source": [
        "## Save Experiment Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqNX17i4c-3l"
      },
      "outputs": [],
      "source": [
        "# Save all experiment results to CSV for reference\n",
        "df_all_results = pd.DataFrame(experiment_results)\n",
        "df_all_results.to_csv('experiment_results.csv', index=False)\n",
        "print(\"Experiment results saved to 'experiment_results.csv'\")\n",
        "\n",
        "# Save the final summary\n",
        "final_summary.to_csv('model_comparison_summary.csv', index=False)\n",
        "print(\"Model comparison saved to 'model_comparison_summary.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlafSnA_c-3l"
      },
      "source": [
        "---\n",
        "# Complete Pipeline for Best Model\n",
        "\n",
        "Below is the complete, reproducible code for the best performing model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO8KItf_c-3m"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "COMPLETE REPRODUCIBLE PIPELINE FOR BEST MODEL\n",
        "=============================================\n",
        "\n",
        "The best model configuration will be determined after running all experiments.\n",
        "This cell will be updated with the exact code to reproduce the best results.\n",
        "\n",
        "Key components:\n",
        "1. Feature engineering (exact transformations)\n",
        "2. Preprocessing pipeline\n",
        "3. Model configuration with optimal hyperparameters\n",
        "4. Training procedure\n",
        "5. Evaluation code\n",
        "\n",
        "Random seed: 42 (for reproducibility)\n",
        "Cross-validation: 3-fold stratified\n",
        "Metric: Custom Payout MSE\n",
        "\n",
        "After running this notebook, copy the best configuration to the main notebook.\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}